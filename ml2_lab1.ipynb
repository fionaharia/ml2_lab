{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **INPUT DATA**\n",
        "\n",
        "The input data for this experiment includes the values for the logic gates, specifically binary inputs (0 or 1) to test the AND, OR, NOT, NAND, NOR, and XOR gates. The binary inputs are combined to simulate the logical operations and determine their outputs. The data is presented in the form of a truth table to clearly display the relationship between the inputs and their corresponding outputs."
      ],
      "metadata": {
        "id": "JOtF75EKRC1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROCEDURE**"
      ],
      "metadata": {
        "id": "Zf4txUfKRyxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize Inputs: Two binary inputs,X1 AND X2 are provided to the perceptron-based logic gate functions.\n",
        "2. AND Gate: The perceptron model is applied using weights [1, 1] and a bias of -1.5 to simulate the AND gate behavior.\n",
        "3. OR Gate: Similarly, the OR gate is simulated using weights [1, 1] and a bias of -0.5.\n",
        "4. NOT Gate: For each input, a NOT gate is implemented using a single weight of [-1] and a bias of 0.5.\n",
        "5. NAND and NOR Gates: These are derived using the NOT gate on the outputs of the AND and OR gates, respectively.\n",
        "6. XOR Gate: The XOR gate is created by combining the NAND and OR gates using the perceptron model.\n",
        "7. Execute: The inputs are then passed through each gate function, and the outputs are calculated and stored.\n",
        "8. Output: Finally, the results are displayed as per the input combinations.\n",
        "\n"
      ],
      "metadata": {
        "id": "CmGirEHSR4DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TECHNOLOGY STACK**"
      ],
      "metadata": {
        "id": "IaktYiRNSbpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python, Numpy and Google Colab"
      ],
      "metadata": {
        "id": "zgU38DugSgDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SOURCE CODE**"
      ],
      "metadata": {
        "id": "BeUqub-fSpwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def perceptron(inputs, weights, bias):\n",
        "    weighted_sum = np.dot(inputs, weights) + bias\n",
        "    return int(weighted_sum >= 0)\n",
        "\n",
        "def and_gate(x1, x2):\n",
        "    weights = np.array([1, 1])\n",
        "    bias = -1.5\n",
        "    inputs = np.array([x1, x2])\n",
        "    print(f\"AND gate - Weights: {weights}, Bias: {bias}\")\n",
        "    return perceptron(inputs, weights, bias)\n",
        "\n",
        "def or_gate(x1, x2):\n",
        "    weights = np.array([1, 1])\n",
        "    bias = -0.5\n",
        "    inputs = np.array([x1, x2])\n",
        "    print(f\"OR gate - Weights: {weights}, Bias: {bias}\")\n",
        "    return perceptron(inputs, weights, bias)\n",
        "\n",
        "def not_gate(x):\n",
        "    weights = np.array([-1])\n",
        "    bias = 0.5\n",
        "    inputs = np.array([x])\n",
        "    print(f\"NOT gate - Weights: {weights}, Bias: {bias}\")\n",
        "    return perceptron(inputs, weights, bias)\n",
        "\n",
        "def nand_gate(x1, x2):\n",
        "    print(f\"NAND gate - NOT of AND gate\")\n",
        "    return not_gate(and_gate(x1, x2))\n",
        "\n",
        "def nor_gate(x1, x2):\n",
        "    print(f\"NOR gate - NOT of OR gate\")\n",
        "    return not_gate(or_gate(x1, x2))\n",
        "\n",
        "def xor_gate(x1, x2):\n",
        "    print(f\"XOR gate - Combination of NAND and OR gates\")\n",
        "    nand_output = nand_gate(x1, x2)\n",
        "    or_output = or_gate(x1, x2)\n",
        "    return and_gate(nand_output, or_output)"
      ],
      "metadata": {
        "id": "CxK6Yh51QhBt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input values\n",
        "x1 = int(input(\"Enter the first input (0 or 1): \"))\n",
        "x2 = int(input(\"Enter the second input (0 or 1): \"))\n",
        "\n",
        "# Calculate and print outputs\n",
        "and_output = and_gate(x1, x2)\n",
        "or_output = or_gate(x1, x2)\n",
        "not_output_x1 = not_gate(x1)\n",
        "not_output_x2 = not_gate(x2)\n",
        "nand_output = nand_gate(x1, x2)\n",
        "nor_output = nor_gate(x1, x2)\n",
        "xor_output = xor_gate(x1, x2)\n",
        "\n",
        "print(f\"AND({x1}, {x2}) = {and_output}\")\n",
        "print(f\"OR({x1}, {x2}) = {or_output}\")\n",
        "print(f\"NOT({x1}) = {not_output_x1}\")\n",
        "print(f\"NOT({x2}) = {not_output_x2}\")\n",
        "print(f\"NAND({x1}, {x2}) = {nand_output}\")\n",
        "print(f\"NOR({x1}, {x2}) = {nor_output}\")\n",
        "print(f\"XOR({x1}, {x2}) = {xor_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSvgiSwZQhU4",
        "outputId": "ab1fd614-8333-4038-e47d-c8f04c5a70ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the first input (0 or 1): 0\n",
            "Enter the second input (0 or 1): 1\n",
            "AND gate - Weights: [1 1], Bias: -1.5\n",
            "OR gate - Weights: [1 1], Bias: -0.5\n",
            "NOT gate - Weights: [-1], Bias: 0.5\n",
            "NOT gate - Weights: [-1], Bias: 0.5\n",
            "NAND gate - NOT of AND gate\n",
            "AND gate - Weights: [1 1], Bias: -1.5\n",
            "NOT gate - Weights: [-1], Bias: 0.5\n",
            "NOR gate - NOT of OR gate\n",
            "OR gate - Weights: [1 1], Bias: -0.5\n",
            "NOT gate - Weights: [-1], Bias: 0.5\n",
            "XOR gate - Combination of NAND and OR gates\n",
            "NAND gate - NOT of AND gate\n",
            "AND gate - Weights: [1 1], Bias: -1.5\n",
            "NOT gate - Weights: [-1], Bias: 0.5\n",
            "OR gate - Weights: [1 1], Bias: -0.5\n",
            "AND gate - Weights: [1 1], Bias: -1.5\n",
            "AND(0, 1) = 0\n",
            "OR(0, 1) = 1\n",
            "NOT(0) = 1\n",
            "NOT(1) = 0\n",
            "NAND(0, 1) = 1\n",
            "NOR(0, 1) = 0\n",
            "XOR(0, 1) = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT ARE THE LIMITATIONS OF THE PERCEPTRON NETWORK**"
      ],
      "metadata": {
        "id": "dsUlSU55Tu6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perceptron network has several limitations, primarily due to its inability to handle non-linearly separable data. It can only solve problems where the data can be divided by a straight line or hyperplane, making it ineffective for tasks that require non-linear decision boundaries, such as the XOR problem. Additionally, being a single-layer model, the perceptron cannot learn more complex patterns or functions, which limits its applicability to simple classification tasks. The use of a step function as the activation function further restricts the perceptron's ability to learn non-linear relationships, as it only allows for binary outputs, thereby missing the nuance required for more sophisticated pattern recognition. These limitations highlight the need for more advanced models, such as multi-layer neural networks, which can overcome these challenges and learn non-linear functions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YXoDWhpsTy2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OBSERVATIONS**"
      ],
      "metadata": {
        "id": "h1cy9qjbSulJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perceptron model accurately simulates the behavior of basic logic gates using appropriate weights and biases. The output of each gate is consistent with expected behavior:\n",
        "\n",
        "1. AND Gate: Only produces a 1 when both inputs are 1.\n",
        "2. OR Gate: Produces a 1 when at least one input is 1.\n",
        "3. NOT Gate: Inverts the input value.\n",
        "4. NAND Gate: Produces the opposite of the AND gate output.\n",
        "5. NOR Gate: Produces the opposite of the OR gate output.\n",
        "6. XOR Gate: Produces a 1 when inputs are different.\n",
        "\n",
        "\n",
        "These results demonstrate how a single-layer perceptron can effectively model simple logical operations. Any deviations from expected outcomes should be investigated by rechecking the weights and bias values."
      ],
      "metadata": {
        "id": "ZBATQD4-Sygb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONCLUSIONS**"
      ],
      "metadata": {
        "id": "px40OWy3TUu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment successfully demonstrates the implementation of logic gates using a perceptron-based model. By adjusting the weights and biases, the perceptron can mimic the behavior of various logical functions, confirming its role as a fundamental building block in neural networks. The results align with theoretical expectations, validating the perceptron's utility in simulating simple logic gates.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rd3g4WeTTXE8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42-c5Y7_TaCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
